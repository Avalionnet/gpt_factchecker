{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Preparation\n",
        "Before training the Google T5 model for our claim decomposition task, it is essential for us to first fetch the necessary training data. \n",
        "\n",
        "This file contains code to reconstruct the ClaimDecomp dataset specified in the `chen-etal-2022-generating ` paper by the University of Texas."
      ],
      "metadata": {
        "id": "5NIzYAK7f7O5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Required Libraries"
      ],
      "metadata": {
        "id": "jaZ9GRmphm7u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I0zD1nNAf1B7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "895289a4-be1d-416a-8dec-5ffd73add28f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.64.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.3.5)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.21.6)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2022.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (4.6.3)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting argparse\n",
            "  Downloading argparse-1.4.0-py2.py3-none-any.whl (23 kB)\n",
            "Installing collected packages: argparse\n",
            "Successfully installed argparse-1.4.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "argparse"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (2.23.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests) (2.10)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting allennlp==2.7\n",
            "  Downloading allennlp-2.7.0-py3-none-any.whl (738 kB)\n",
            "\u001b[K     |████████████████████████████████| 738 kB 7.7 MB/s \n",
            "\u001b[?25hCollecting wandb<0.13.0,>=0.10.0\n",
            "  Downloading wandb-0.12.21-py2.py3-none-any.whl (1.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 38.2 MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 52.2 MB/s \n",
            "\u001b[?25hCollecting boto3<2.0,>=1.14\n",
            "  Downloading boto3-1.24.95-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 62.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.18 in /usr/local/lib/python3.7/dist-packages (from allennlp==2.7) (2.23.0)\n",
            "Collecting torch<1.10.0,>=1.6.0\n",
            "  Downloading torch-1.9.1-cp37-cp37m-manylinux1_x86_64.whl (831.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 831.4 MB 6.6 kB/s \n",
            "\u001b[?25hRequirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (from allennlp==2.7) (3.6.4)\n",
            "Collecting datasets<2.0,>=1.2.1\n",
            "  Downloading datasets-1.18.4-py3-none-any.whl (312 kB)\n",
            "\u001b[K     |████████████████████████████████| 312 kB 53.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: more-itertools in /usr/local/lib/python3.7/dist-packages (from allennlp==2.7) (8.14.0)\n",
            "Collecting tensorboardX>=1.2\n",
            "  Downloading tensorboardX-2.5.1-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[K     |████████████████████████████████| 125 kB 45.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from allennlp==2.7) (0.3.5.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from allennlp==2.7) (3.7)\n",
            "Collecting jsonnet>=0.10.0\n",
            "  Downloading jsonnet-0.18.0.tar.gz (592 kB)\n",
            "\u001b[K     |████████████████████████████████| 592 kB 42.7 MB/s \n",
            "\u001b[?25hCollecting sqlitedict\n",
            "  Downloading sqlitedict-2.0.0.tar.gz (46 kB)\n",
            "\u001b[K     |████████████████████████████████| 46 kB 4.1 MB/s \n",
            "\u001b[?25hCollecting filelock<3.1,>=3.0\n",
            "  Downloading filelock-3.0.12-py3-none-any.whl (7.6 kB)\n",
            "Collecting google-cloud-storage<1.43.0,>=1.38.0\n",
            "  Downloading google_cloud_storage-1.42.3-py2.py3-none-any.whl (105 kB)\n",
            "\u001b[K     |████████████████████████████████| 105 kB 53.5 MB/s \n",
            "\u001b[?25hCollecting base58\n",
            "  Downloading base58-2.1.1-py3-none-any.whl (5.6 kB)\n",
            "Collecting transformers<4.10,>=4.1\n",
            "  Downloading transformers-4.9.2-py3-none-any.whl (2.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6 MB 32.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: lmdb in /usr/local/lib/python3.7/dist-packages (from allennlp==2.7) (0.99)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from allennlp==2.7) (1.7.3)\n",
            "Collecting fairscale==0.4.0\n",
            "  Downloading fairscale-0.4.0.tar.gz (190 kB)\n",
            "\u001b[K     |████████████████████████████████| 190 kB 54.5 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from allennlp==2.7) (3.1.0)\n",
            "Collecting huggingface-hub>=0.0.8\n",
            "  Downloading huggingface_hub-0.10.1-py3-none-any.whl (163 kB)\n",
            "\u001b[K     |████████████████████████████████| 163 kB 42.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.19 in /usr/local/lib/python3.7/dist-packages (from allennlp==2.7) (4.64.1)\n",
            "Collecting checklist==0.0.11\n",
            "  Downloading checklist-0.0.11.tar.gz (12.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.1 MB 46.9 MB/s \n",
            "\u001b[?25hCollecting termcolor==1.1.0\n",
            "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
            "Collecting torchvision<0.11.0,>=0.8.1\n",
            "  Downloading torchvision-0.10.1-cp37-cp37m-manylinux1_x86_64.whl (22.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 22.1 MB 1.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from allennlp==2.7) (1.0.2)\n",
            "Collecting spacy<3.2,>=2.1.0\n",
            "  Downloading spacy-3.1.6-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.9 MB 32.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from allennlp==2.7) (1.21.6)\n",
            "Collecting overrides==3.1.0\n",
            "  Downloading overrides-3.1.0.tar.gz (11 kB)\n",
            "Collecting munch>=2.5\n",
            "  Downloading munch-2.5.0-py2.py3-none-any.whl (10 kB)\n",
            "Collecting jupyter>=1.0\n",
            "  Using cached jupyter-1.0.0-py2.py3-none-any.whl (2.7 kB)\n",
            "Requirement already satisfied: ipywidgets>=7.5 in /usr/local/lib/python3.7/dist-packages (from checklist==0.0.11->allennlp==2.7) (7.7.1)\n",
            "Collecting patternfork-nosql\n",
            "  Downloading patternfork_nosql-3.6.tar.gz (22.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 22.3 MB 1.2 MB/s \n",
            "\u001b[?25hCollecting iso-639\n",
            "  Downloading iso-639-0.4.5.tar.gz (167 kB)\n",
            "\u001b[K     |████████████████████████████████| 167 kB 64.0 MB/s \n",
            "\u001b[?25hCollecting s3transfer<0.7.0,>=0.6.0\n",
            "  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 8.2 MB/s \n",
            "\u001b[?25hCollecting botocore<1.28.0,>=1.27.95\n",
            "  Downloading botocore-1.27.95-py3-none-any.whl (9.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.3 MB 28.8 MB/s \n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting urllib3<1.27,>=1.25.4\n",
            "  Downloading urllib3-1.26.12-py2.py3-none-any.whl (140 kB)\n",
            "\u001b[K     |████████████████████████████████| 140 kB 55.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.28.0,>=1.27.95->boto3<2.0,>=1.14->allennlp==2.7) (2.8.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets<2.0,>=1.2.1->allennlp==2.7) (3.8.3)\n",
            "Requirement already satisfied: pyarrow!=4.0.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets<2.0,>=1.2.1->allennlp==2.7) (6.0.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets<2.0,>=1.2.1->allennlp==2.7) (1.3.5)\n",
            "Collecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.1.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[K     |████████████████████████████████| 212 kB 39.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from datasets<2.0,>=1.2.1->allennlp==2.7) (2022.8.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets<2.0,>=1.2.1->allennlp==2.7) (4.13.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets<2.0,>=1.2.1->allennlp==2.7) (21.3)\n",
            "Collecting multiprocess\n",
            "  Downloading multiprocess-0.70.13-py37-none-any.whl (115 kB)\n",
            "\u001b[K     |████████████████████████████████| 115 kB 72.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets<2.0,>=1.2.1->allennlp==2.7) (2.1.1)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets<2.0,>=1.2.1->allennlp==2.7) (0.13.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets<2.0,>=1.2.1->allennlp==2.7) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets<2.0,>=1.2.1->allennlp==2.7) (22.1.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets<2.0,>=1.2.1->allennlp==2.7) (4.0.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets<2.0,>=1.2.1->allennlp==2.7) (6.0.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets<2.0,>=1.2.1->allennlp==2.7) (1.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets<2.0,>=1.2.1->allennlp==2.7) (4.1.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets<2.0,>=1.2.1->allennlp==2.7) (1.8.1)\n",
            "Collecting google-cloud-core<3.0dev,>=1.6.0\n",
            "  Downloading google_cloud_core-2.3.2-py2.py3-none-any.whl (29 kB)\n",
            "Requirement already satisfied: google-auth<3.0dev,>=1.25.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage<1.43.0,>=1.38.0->allennlp==2.7) (1.35.0)\n",
            "Collecting google-resumable-media<3.0dev,>=1.3.0\n",
            "  Downloading google_resumable_media-2.4.0-py2.py3-none-any.whl (77 kB)\n",
            "\u001b[K     |████████████████████████████████| 77 kB 2.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-api-core<3.0dev,>=1.29.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage<1.43.0,>=1.38.0->allennlp==2.7) (1.31.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage<1.43.0,>=1.38.0->allennlp==2.7) (1.15.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage<1.43.0,>=1.38.0->allennlp==2.7) (3.17.3)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3.0dev,>=1.29.0->google-cloud-storage<1.43.0,>=1.38.0->allennlp==2.7) (1.56.4)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3.0dev,>=1.29.0->google-cloud-storage<1.43.0,>=1.38.0->allennlp==2.7) (57.4.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core<3.0dev,>=1.29.0->google-cloud-storage<1.43.0,>=1.38.0->allennlp==2.7) (2022.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage<1.43.0,>=1.38.0->allennlp==2.7) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage<1.43.0,>=1.38.0->allennlp==2.7) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage<1.43.0,>=1.38.0->allennlp==2.7) (4.2.4)\n",
            "Collecting google-crc32c<2.0dev,>=1.0\n",
            "  Downloading google_crc32c-1.5.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32 kB)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.8->allennlp==2.7) (6.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5->checklist==0.0.11->allennlp==2.7) (5.1.1)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5->checklist==0.0.11->allennlp==2.7) (7.9.0)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5->checklist==0.0.11->allennlp==2.7) (5.3.4)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5->checklist==0.0.11->allennlp==2.7) (0.2.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5->checklist==0.0.11->allennlp==2.7) (3.6.1)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5->checklist==0.0.11->allennlp==2.7) (3.0.3)\n",
            "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.5->checklist==0.0.11->allennlp==2.7) (5.1.1)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.5->checklist==0.0.11->allennlp==2.7) (6.1.12)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5->checklist==0.0.11->allennlp==2.7) (4.8.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5->checklist==0.0.11->allennlp==2.7) (2.0.10)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5->checklist==0.0.11->allennlp==2.7) (2.6.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5->checklist==0.0.11->allennlp==2.7) (0.7.5)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5->checklist==0.0.11->allennlp==2.7) (0.2.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5->checklist==0.0.11->allennlp==2.7) (4.4.2)\n",
            "Collecting jedi>=0.10\n",
            "  Using cached jedi-0.18.1-py2.py3-none-any.whl (1.6 MB)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.10->ipython>=4.0.0->ipywidgets>=7.5->checklist==0.0.11->allennlp==2.7) (0.8.3)\n",
            "Collecting qtconsole\n",
            "  Using cached qtconsole-5.3.2-py3-none-any.whl (120 kB)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.7/dist-packages (from jupyter>=1.0->checklist==0.0.11->allennlp==2.7) (6.1.0)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.7/dist-packages (from jupyter>=1.0->checklist==0.0.11->allennlp==2.7) (5.5.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from jupyter>=1.0->checklist==0.0.11->allennlp==2.7) (5.6.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets<2.0,>=1.2.1->allennlp==2.7) (3.0.9)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets>=7.5->checklist==0.0.11->allennlp==2.7) (0.2.5)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-cloud-storage<1.43.0,>=1.38.0->allennlp==2.7) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp==2.7) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp==2.7) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp==2.7) (2.10)\n",
            "Collecting urllib3<1.27,>=1.25.4\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 44.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp==2.7) (2.11.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp==2.7) (0.10.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp==2.7) (2.0.7)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp==2.7) (2.0.8)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp==2.7) (0.6.2)\n",
            "Collecting typing-extensions>=3.7.4\n",
            "  Downloading typing_extensions-3.10.0.2-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp==2.7) (0.7.8)\n",
            "Collecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4\n",
            "  Downloading pydantic-1.8.2-cp37-cp37m-manylinux2014_x86_64.whl (10.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.1 MB 27.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp==2.7) (1.0.9)\n",
            "Collecting thinc<8.1.0,>=8.0.12\n",
            "  Downloading thinc-8.0.17-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (660 kB)\n",
            "\u001b[K     |████████████████████████████████| 660 kB 61.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp==2.7) (0.4.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp==2.7) (2.4.4)\n",
            "Requirement already satisfied: click<8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp==2.7) (7.1.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp==2.7) (3.0.8)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp==2.7) (3.0.10)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.2,>=2.1.0->allennlp==2.7) (3.9.0)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.2,>=2.1.0->allennlp==2.7) (5.2.1)\n",
            "Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision<0.11.0,>=0.8.1->allennlp==2.7) (7.1.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<4.10,>=4.1->allennlp==2.7) (2022.6.2)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 41.1 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[K     |████████████████████████████████| 880 kB 58.1 MB/s \n",
            "\u001b[?25hCollecting transformers<4.10,>=4.1\n",
            "  Downloading transformers-4.9.1-py3-none-any.whl (2.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6 MB 44.4 MB/s \n",
            "\u001b[?25h  Downloading transformers-4.9.0-py3-none-any.whl (2.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6 MB 42.4 MB/s \n",
            "\u001b[?25h  Downloading transformers-4.8.2-py3-none-any.whl (2.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.5 MB 24.4 MB/s \n",
            "\u001b[?25h  Downloading transformers-4.8.1-py3-none-any.whl (2.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.5 MB 42.3 MB/s \n",
            "\u001b[?25h  Downloading transformers-4.8.0-py3-none-any.whl (2.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.5 MB 44.0 MB/s \n",
            "\u001b[?25h  Downloading transformers-4.7.0-py3-none-any.whl (2.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.5 MB 43.9 MB/s \n",
            "\u001b[?25h  Downloading transformers-4.6.1-py3-none-any.whl (2.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2 MB 40.6 MB/s \n",
            "\u001b[?25h  Downloading transformers-4.6.0-py3-none-any.whl (2.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3 MB 28.4 MB/s \n",
            "\u001b[?25h  Downloading transformers-4.5.1-py3-none-any.whl (2.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1 MB 49.6 MB/s \n",
            "\u001b[?25hCollecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.10.0-py2.py3-none-any.whl (166 kB)\n",
            "\u001b[K     |████████████████████████████████| 166 kB 67.9 MB/s \n",
            "\u001b[?25hCollecting setproctitle\n",
            "  Downloading setproctitle-1.3.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.9-py3-none-any.whl (9.4 kB)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp==2.7) (5.4.8)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.29-py3-none-any.whl (182 kB)\n",
            "\u001b[K     |████████████████████████████████| 182 kB 49.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp==2.7) (2.3)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 2.0 MB/s \n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.9.10-py2.py3-none-any.whl (162 kB)\n",
            "\u001b[K     |████████████████████████████████| 162 kB 55.7 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.9-py2.py3-none-any.whl (162 kB)\n",
            "\u001b[K     |████████████████████████████████| 162 kB 51.2 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.8-py2.py3-none-any.whl (158 kB)\n",
            "\u001b[K     |████████████████████████████████| 158 kB 53.8 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.7-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 54.3 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.6-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 55.9 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.5-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 54.6 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.4-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 43.8 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.3-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 59.9 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.2-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 53.8 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.1-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 47.6 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.0-py2.py3-none-any.whl (156 kB)\n",
            "\u001b[K     |████████████████████████████████| 156 kB 46.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter>=1.0->checklist==0.0.11->allennlp==2.7) (1.8.0)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter>=1.0->checklist==0.0.11->allennlp==2.7) (23.2.1)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter>=1.0->checklist==0.0.11->allennlp==2.7) (0.13.3)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter>=1.0->checklist==0.0.11->allennlp==2.7) (5.7.0)\n",
            "Requirement already satisfied: jupyter-core>=4.4.0 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter>=1.0->checklist==0.0.11->allennlp==2.7) (4.11.1)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.7/dist-packages (from terminado>=0.8.1->notebook->jupyter>=1.0->checklist==0.0.11->allennlp==2.7) (0.7.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->allennlp==2.7) (1.5.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.2,>=2.1.0->allennlp==2.7) (2.0.1)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0->checklist==0.0.11->allennlp==2.7) (0.4)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0->checklist==0.0.11->allennlp==2.7) (0.7.1)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0->checklist==0.0.11->allennlp==2.7) (0.6.0)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0->checklist==0.0.11->allennlp==2.7) (0.8.4)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0->checklist==0.0.11->allennlp==2.7) (5.0.1)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0->checklist==0.0.11->allennlp==2.7) (1.5.0)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.7/dist-packages (from nbformat->notebook->jupyter>=1.0->checklist==0.0.11->allennlp==2.7) (4.3.3)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.7/dist-packages (from nbformat->notebook->jupyter>=1.0->checklist==0.0.11->allennlp==2.7) (2.16.2)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->notebook->jupyter>=1.0->checklist==0.0.11->allennlp==2.7) (5.10.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->notebook->jupyter>=1.0->checklist==0.0.11->allennlp==2.7) (0.18.1)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->jupyter>=1.0->checklist==0.0.11->allennlp==2.7) (0.5.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk->allennlp==2.7) (1.2.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from patternfork-nosql->checklist==0.0.11->allennlp==2.7) (0.16.0)\n",
            "Collecting backports.csv\n",
            "  Downloading backports.csv-1.0.7-py2.py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from patternfork-nosql->checklist==0.0.11->allennlp==2.7) (4.6.3)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from patternfork-nosql->checklist==0.0.11->allennlp==2.7) (4.9.1)\n",
            "Collecting feedparser\n",
            "  Downloading feedparser-6.0.10-py3-none-any.whl (81 kB)\n",
            "\u001b[K     |████████████████████████████████| 81 kB 6.2 MB/s \n",
            "\u001b[?25hCollecting pdfminer.six\n",
            "  Downloading pdfminer.six-20220524-py3-none-any.whl (5.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6 MB 23.5 MB/s \n",
            "\u001b[?25hCollecting python-docx\n",
            "  Downloading python-docx-0.8.11.tar.gz (5.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6 MB 35.6 MB/s \n",
            "\u001b[?25hCollecting cherrypy\n",
            "  Downloading CherryPy-18.8.0-py2.py3-none-any.whl (348 kB)\n",
            "\u001b[K     |████████████████████████████████| 348 kB 52.9 MB/s \n",
            "\u001b[?25hCollecting cheroot>=8.2.1\n",
            "  Downloading cheroot-8.6.0-py2.py3-none-any.whl (104 kB)\n",
            "\u001b[K     |████████████████████████████████| 104 kB 54.1 MB/s \n",
            "\u001b[?25hCollecting zc.lockfile\n",
            "  Downloading zc.lockfile-2.0-py2.py3-none-any.whl (9.7 kB)\n",
            "Collecting portend>=2.1.1\n",
            "  Downloading portend-3.1.0-py3-none-any.whl (5.3 kB)\n",
            "Collecting jaraco.collections\n",
            "  Downloading jaraco.collections-3.5.2-py3-none-any.whl (10 kB)\n",
            "Collecting jaraco.functools\n",
            "  Downloading jaraco.functools-3.5.2-py3-none-any.whl (7.3 kB)\n",
            "Collecting tempora>=1.8\n",
            "  Downloading tempora-5.0.2-py3-none-any.whl (15 kB)\n",
            "Collecting sgmllib3k\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "Collecting jaraco.classes\n",
            "  Downloading jaraco.classes-3.2.3-py3-none-any.whl (6.0 kB)\n",
            "Collecting jaraco.text\n",
            "  Downloading jaraco.text-3.10.0-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: inflect in /usr/local/lib/python3.7/dist-packages (from jaraco.text->jaraco.collections->cherrypy->patternfork-nosql->checklist==0.0.11->allennlp==2.7) (2.1.0)\n",
            "Collecting autocommand\n",
            "  Downloading autocommand-2.2.1-py3-none-any.whl (22 kB)\n",
            "Collecting jaraco.context>=4.1\n",
            "  Downloading jaraco.context-4.1.2-py3-none-any.whl (4.7 kB)\n",
            "Collecting cryptography>=36.0.0\n",
            "  Downloading cryptography-38.0.1-cp36-abi3-manylinux_2_24_x86_64.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 43.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=36.0.0->pdfminer.six->patternfork-nosql->checklist==0.0.11->allennlp==2.7) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six->patternfork-nosql->checklist==0.0.11->allennlp==2.7) (2.21)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp==2.7) (1.4.1)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp==2.7) (0.7.1)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp==2.7) (1.11.0)\n",
            "Collecting qtpy>=2.0.1\n",
            "  Using cached QtPy-2.2.1-py3-none-any.whl (82 kB)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->allennlp==2.7) (3.1.0)\n",
            "Building wheels for collected packages: checklist, fairscale, overrides, termcolor, jsonnet, iso-639, pathtools, patternfork-nosql, python-docx, sacremoses, sgmllib3k, sqlitedict\n",
            "  Building wheel for checklist (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for checklist: filename=checklist-0.0.11-py3-none-any.whl size=12165635 sha256=d60f3766be223831fe32282bc15e6e8fa2f1fe905caf6452803581751e7679d7\n",
            "  Stored in directory: /root/.cache/pip/wheels/6a/8a/07/6446879be434879c27671c83443727d74cecf6b630c8a24d03\n",
            "  Building wheel for fairscale (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairscale: filename=fairscale-0.4.0-py3-none-any.whl size=239949 sha256=21d8f1c830294fccf12a317a154b586936911306e99ece22d52c54fdc560736c\n",
            "  Stored in directory: /root/.cache/pip/wheels/28/8e/a3/7a2f33ac996114b816d88e55cf1235a1e058f30211e39bd719\n",
            "  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for overrides: filename=overrides-3.1.0-py3-none-any.whl size=10187 sha256=175035dd48d683a428c485ac2f61d9a1a9d82f9aadd34094cdd508385b9a9b8f\n",
            "  Stored in directory: /root/.cache/pip/wheels/3a/0d/38/01a9bc6e20dcfaf0a6a7b552d03137558ba1c38aea47644682\n",
            "  Building wheel for termcolor (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4848 sha256=45bcb59d945129d76dd9b54cbb282d578091933c6cd731fd6f11b7223382f651\n",
            "  Stored in directory: /root/.cache/pip/wheels/3f/e3/ec/8a8336ff196023622fbcb36de0c5a5c218cbb24111d1d4c7f2\n",
            "  Building wheel for jsonnet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jsonnet: filename=jsonnet-0.18.0-cp37-cp37m-linux_x86_64.whl size=3994706 sha256=6d0bd8259f8181038111a6cd6c0995aaeb7c88e0c0911ac4b3e6ae259cd3a4d2\n",
            "  Stored in directory: /root/.cache/pip/wheels/a9/63/f9/a653f9c21575e6ff271ee6a49939aa002005174cea6c35919d\n",
            "  Building wheel for iso-639 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for iso-639: filename=iso_639-0.4.5-py3-none-any.whl size=169061 sha256=6be019f1c797667b79afe7366d0fdbca50c8187c3c11a2f07c4c80b04651e78b\n",
            "  Stored in directory: /root/.cache/pip/wheels/47/60/19/6d020fc92138ed1b113a18271e83ea4b5525fe770cb45b9a2e\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=ee61ae92618303db597dcce45a357c67e53d874c8806308eed63a024b3a885d7\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "  Building wheel for patternfork-nosql (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for patternfork-nosql: filename=patternfork_nosql-3.6-py3-none-any.whl size=22332804 sha256=7a908ead3b67271ae1b71866b0984c25ab3b7a757fb52d7ef9fd30117da0e986\n",
            "  Stored in directory: /root/.cache/pip/wheels/97/72/8f/5305fe28168f93b658da9ed433b9a1d3ec90594faa0c9aaf4b\n",
            "  Building wheel for python-docx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-docx: filename=python_docx-0.8.11-py3-none-any.whl size=184507 sha256=3825b783a4d09e5e4a8df1c2b610ba8325036bce667d1907e931f94c6b014720\n",
            "  Stored in directory: /root/.cache/pip/wheels/f6/6f/b9/d798122a8b55b74ad30b5f52b01482169b445fbb84a11797a6\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=5e9cdf467c717691477d8d0de30f59838706ce71e3b4c0f8335060ec2b97bf61\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6066 sha256=1732ee2da3a6a8322a0b4d52034e261074273e4ed0d8d377331e3bfa29df7a01\n",
            "  Stored in directory: /root/.cache/pip/wheels/73/ad/a4/0dff4a6ef231fc0dfa12ffbac2a36cebfdddfe059f50e019aa\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sqlitedict: filename=sqlitedict-2.0.0-py3-none-any.whl size=15736 sha256=fc815568202faafe9acebc44a6d68a4ff27d52378741664033f298cb196abf05\n",
            "  Stored in directory: /root/.cache/pip/wheels/96/dd/2e/0ed4a25cb73fc30c7ea8d10b50acb7226175736067e40a7ea3\n",
            "Successfully built checklist fairscale overrides termcolor jsonnet iso-639 pathtools patternfork-nosql python-docx sacremoses sgmllib3k sqlitedict\n",
            "Installing collected packages: typing-extensions, jedi, jaraco.functools, jaraco.context, autocommand, urllib3, tempora, jaraco.text, jaraco.classes, zc.lockfile, smmap, sgmllib3k, qtpy, pydantic, portend, jmespath, jaraco.collections, cryptography, cheroot, tokenizers, thinc, sacremoses, qtconsole, python-docx, pdfminer.six, google-crc32c, gitdb, filelock, feedparser, cherrypy, botocore, backports.csv, xxhash, transformers, torch, spacy, shortuuid, setproctitle, sentry-sdk, s3transfer, responses, patternfork-nosql, pathtools, munch, multiprocess, jupyter, iso-639, huggingface-hub, google-resumable-media, google-cloud-core, GitPython, docker-pycreds, wandb, torchvision, termcolor, tensorboardX, sqlitedict, sentencepiece, overrides, jsonnet, google-cloud-storage, fairscale, datasets, checklist, boto3, base58, allennlp\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing-extensions 4.1.1\n",
            "    Uninstalling typing-extensions-4.1.1:\n",
            "      Successfully uninstalled typing-extensions-4.1.1\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 1.9.2\n",
            "    Uninstalling pydantic-1.9.2:\n",
            "      Successfully uninstalled pydantic-1.9.2\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 8.1.4\n",
            "    Uninstalling thinc-8.1.4:\n",
            "      Successfully uninstalled thinc-8.1.4\n",
            "  Attempting uninstall: filelock\n",
            "    Found existing installation: filelock 3.8.0\n",
            "    Uninstalling filelock-3.8.0:\n",
            "      Successfully uninstalled filelock-3.8.0\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.12.1+cu113\n",
            "    Uninstalling torch-1.12.1+cu113:\n",
            "      Successfully uninstalled torch-1.12.1+cu113\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.4.1\n",
            "    Uninstalling spacy-3.4.1:\n",
            "      Successfully uninstalled spacy-3.4.1\n",
            "  Attempting uninstall: google-resumable-media\n",
            "    Found existing installation: google-resumable-media 0.4.1\n",
            "    Uninstalling google-resumable-media-0.4.1:\n",
            "      Successfully uninstalled google-resumable-media-0.4.1\n",
            "  Attempting uninstall: google-cloud-core\n",
            "    Found existing installation: google-cloud-core 1.0.3\n",
            "    Uninstalling google-cloud-core-1.0.3:\n",
            "      Successfully uninstalled google-cloud-core-1.0.3\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.13.1+cu113\n",
            "    Uninstalling torchvision-0.13.1+cu113:\n",
            "      Successfully uninstalled torchvision-0.13.1+cu113\n",
            "  Attempting uninstall: termcolor\n",
            "    Found existing installation: termcolor 2.0.1\n",
            "    Uninstalling termcolor-2.0.1:\n",
            "      Successfully uninstalled termcolor-2.0.1\n",
            "  Attempting uninstall: google-cloud-storage\n",
            "    Found existing installation: google-cloud-storage 1.18.1\n",
            "    Uninstalling google-cloud-storage-1.18.1:\n",
            "      Successfully uninstalled google-cloud-storage-1.18.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.13.1 requires torch==1.12.1, but you have torch 1.9.1 which is incompatible.\n",
            "torchaudio 0.12.1+cu113 requires torch==1.12.1, but you have torch 1.9.1 which is incompatible.\n",
            "google-cloud-translate 1.5.0 requires google-cloud-core<2.0dev,>=1.0.0, but you have google-cloud-core 2.3.2 which is incompatible.\n",
            "google-cloud-firestore 1.7.0 requires google-cloud-core<2.0dev,>=1.0.3, but you have google-cloud-core 2.3.2 which is incompatible.\n",
            "google-cloud-datastore 1.8.0 requires google-cloud-core<2.0dev,>=1.0.0, but you have google-cloud-core 2.3.2 which is incompatible.\n",
            "google-cloud-bigquery 1.21.0 requires google-cloud-core<2.0dev,>=1.0.3, but you have google-cloud-core 2.3.2 which is incompatible.\n",
            "google-cloud-bigquery 1.21.0 requires google-resumable-media!=0.4.0,<0.5.0dev,>=0.3.1, but you have google-resumable-media 2.4.0 which is incompatible.\n",
            "en-core-web-sm 3.4.0 requires spacy<3.5.0,>=3.4.0, but you have spacy 3.1.6 which is incompatible.\u001b[0m\n",
            "Successfully installed GitPython-3.1.29 allennlp-2.7.0 autocommand-2.2.1 backports.csv-1.0.7 base58-2.1.1 boto3-1.24.95 botocore-1.27.95 checklist-0.0.11 cheroot-8.6.0 cherrypy-18.8.0 cryptography-38.0.1 datasets-1.18.4 docker-pycreds-0.4.0 fairscale-0.4.0 feedparser-6.0.10 filelock-3.0.12 gitdb-4.0.9 google-cloud-core-2.3.2 google-cloud-storage-1.42.3 google-crc32c-1.5.0 google-resumable-media-2.4.0 huggingface-hub-0.10.1 iso-639-0.4.5 jaraco.classes-3.2.3 jaraco.collections-3.5.2 jaraco.context-4.1.2 jaraco.functools-3.5.2 jaraco.text-3.10.0 jedi-0.18.1 jmespath-1.0.1 jsonnet-0.18.0 jupyter-1.0.0 multiprocess-0.70.13 munch-2.5.0 overrides-3.1.0 pathtools-0.1.2 patternfork-nosql-3.6 pdfminer.six-20220524 portend-3.1.0 pydantic-1.8.2 python-docx-0.8.11 qtconsole-5.3.2 qtpy-2.2.1 responses-0.18.0 s3transfer-0.6.0 sacremoses-0.0.53 sentencepiece-0.1.97 sentry-sdk-1.9.0 setproctitle-1.3.2 sgmllib3k-1.0.0 shortuuid-1.0.9 smmap-5.0.0 spacy-3.1.6 sqlitedict-2.0.0 tempora-5.0.2 tensorboardX-2.5.1 termcolor-1.1.0 thinc-8.0.17 tokenizers-0.10.3 torch-1.9.1 torchvision-0.10.1 transformers-4.5.1 typing-extensions-3.10.0.2 urllib3-1.25.11 wandb-0.12.21 xxhash-3.1.0 zc.lockfile-2.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google",
                  "typing_extensions"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torch==1.9.0\n",
            "  Downloading torch-1.9.0-cp37-cp37m-manylinux1_x86_64.whl (831.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 831.4 MB 2.1 kB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.9.0) (3.10.0.2)\n",
            "Installing collected packages: torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.9.1\n",
            "    Uninstalling torch-1.9.1:\n",
            "      Successfully uninstalled torch-1.9.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.10.1 requires torch==1.9.1, but you have torch 1.9.0 which is incompatible.\n",
            "torchtext 0.13.1 requires torch==1.12.1, but you have torch 1.9.0 which is incompatible.\n",
            "torchaudio 0.12.1+cu113 requires torch==1.12.1, but you have torch 1.9.0 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install tqdm\n",
        "!pip install pandas\n",
        "!pip install beautifulsoup4\n",
        "!pip install argparse\n",
        "!pip install requests\n",
        "!pip install allennlp==2.7\n",
        "!pip install torch==1.9.0"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Download ClaimDecomp Dataset from the University of Texas"
      ],
      "metadata": {
        "id": "suifXQcrj4Q3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O ./claim_decomp_raw/train.jsonl 'https://www.cs.utexas.edu/~jfchen/claim-decomp/train.jsonl'\n",
        "!wget -O ./claim_decomp_raw/dev.jsonl 'https://www.cs.utexas.edu/~jfchen/claim-decomp/dev.jsonl'\n",
        "!wget -O ./claim_decomp_raw/test.jsonl 'https://www.cs.utexas.edu/~jfchen/claim-decomp/test.jsonl'"
      ],
      "metadata": {
        "id": "2I01wXWDlB0r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reconstruct ClaimDecomp Dataset\n",
        "\n",
        "As the inital dataset downloaded from ClaimDecomp contains incomplete fields (like missing claims, person etc), the data has to be reconstructed by processing information from the source articles of each claim"
      ],
      "metadata": {
        "id": "n2FGVAH1UR2s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Information on Train.jsonl\n",
        "\n",
        "Removed Invalid Samples: \n",
        "* **717-719**\n",
        "\n",
        "Skipped Samples: \n",
        "1. 241\n",
        "2. 513\n",
        "3. 579\n",
        "4. 586\n",
        "\n",
        "Total Samples: \n",
        "**793**\n",
        "\n"
      ],
      "metadata": {
        "id": "XTWquaHTvbwi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 reconstruct_dataset.py --input_path ./claim_decomp_raw/train.jsonl --output ./Reconstructed/train.jsonl"
      ],
      "metadata": {
        "id": "swm1dtqAg7na"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Information on dev.jsonl\n",
        "\n",
        "Removed Invalid Samples: **34-36**\n",
        "\n",
        "Total Samples: **197**"
      ],
      "metadata": {
        "id": "y7LISo4aoqxe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 reconstruct_dataset.py --input_path ./claim_decomp_raw/dev.jsonl --output ./Reconstructed/dev.jsonl"
      ],
      "metadata": {
        "id": "AkdA8Yihjkjo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91b459a2-06b9-4cdb-985c-84d298b4734f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of Dataframe: 197\n",
            "\n",
            "197it [05:15,  1.60s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Information on test.jsonl\n",
        "\n",
        "Total Samples: **200**"
      ],
      "metadata": {
        "id": "s9OX1fiuVA8m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 reconstruct_dataset.py --input_path ./claim_decomp_raw/test.jsonl --output ./Reconstructed/test.jsonl"
      ],
      "metadata": {
        "id": "qn6qyuNrkLcc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7aaf4d1b-2248-4499-941f-7f10b77a07e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of Dataframe: 200\n",
            "\n",
            "200it [06:26,  1.93s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataframe Creation\n",
        "\n",
        "The following code serves to construct a Pandas dataframe from the raw jsonl files reconstructed from the original ClaimDecomp dataset"
      ],
      "metadata": {
        "id": "1MCCHdtMPGcE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "from pandas import json_normalize\n",
        "\n",
        "path = \"./reconstructed_data/dev.jsonl\"\n",
        "with open(path, 'r') as jsonl_file:\n",
        "    jsonl_list = list(jsonl_file)\n",
        "\n",
        "#listOfRows is a list of dict, each containing data for each row in the df with keys = column name\n",
        "listOfRows = []\n",
        "for entry in jsonl_list:\n",
        "    loadedEntry = json.loads(entry)\n",
        "    # print(f\"result: {loadedEntry}\")\n",
        "    # print(isinstance(loadedEntry, dict))\n",
        "    listOfRows.append(loadedEntry)\n",
        "\n",
        "df = pd.DataFrame(listOfRows)\n",
        "df.head()\n",
        "\n",
        "# print(df['claim'][0])\n",
        "# df['annotations'][0][1]['questions']\n",
        "# df.iloc[0]['annotations']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 597
        },
        "id": "NkBXLBgCLxNs",
        "outputId": "aed5c8c6-0d2d-463a-cec1-5d5793dfc51b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            example_id        label  \\\n",
              "0  8057719209342304749        false   \n",
              "1 -3333998957238197422  barely-true   \n",
              "2 -5816336384767541299  barely-true   \n",
              "3  7968458905312541095         true   \n",
              "4 -2095875040468818200        false   \n",
              "\n",
              "                                                 url  \\\n",
              "0  https://www.politifact.com/factchecks/2020/apr...   \n",
              "1  https://www.politifact.com/factchecks/2019/mar...   \n",
              "2  https://www.politifact.com/factchecks/2016/nov...   \n",
              "3  https://www.politifact.com/factchecks/2014/dec...   \n",
              "4  https://www.politifact.com/factchecks/2020/sep...   \n",
              "\n",
              "                                         annotations  \\\n",
              "0  [{'questions': ['Is voting fraud widespread in...   \n",
              "1  [{'questions': ['Was the federal aid given by ...   \n",
              "2  [{'questions': ['Is this ban directly linked t...   \n",
              "3  [{'questions': ['Is it true that  The United S...   \n",
              "4  [{'questions': ['Has Trump been accused of wal...   \n",
              "\n",
              "                                               claim              person  \\\n",
              "0  With voting by mail, “you get thousands and th...        Donald Trump   \n",
              "1  \"I’ve already traveled to Washington, D.C., an...        Ron DeSantis   \n",
              "2  Says that when San Francisco banned plastic gr...      James Quintero   \n",
              "3  The United States \"decided waterboarding was t...  Sheldon Whitehouse   \n",
              "4  Quotes Donald Trump as saying, “I’ll tell you ...         Viral image   \n",
              "\n",
              "                                               venue  \\\n",
              "0       stated on April 7, 2020 in a press briefing:   \n",
              "1  stated on March 5, 2019 in his State of the St...   \n",
              "2  stated on October 10, 2016 in a panel discussi...   \n",
              "3     stated on December 14, 2014 in a TV interview:   \n",
              "4  stated on September 21, 2020 in a post on Face...   \n",
              "\n",
              "                                       justification  \\\n",
              "0  Trump said that with voting by mail, \"you get ...   \n",
              "1  DeSantis said, \"I’ve already traveled to Washi...   \n",
              "2  Quintero said that when San Francisco banned p...   \n",
              "3  Sheldon Whitehouse said the United States \"dec...   \n",
              "4  A Facebook post quotes President Trump as sayi...   \n",
              "\n",
              "                                        full_article  \n",
              "0  The daily White House briefings about coronavi...  \n",
              "1  Editor’s note, March 10 12:55 p.m.: Two days a...  \n",
              "2  Reused grocery bags made Californians sick, a ...  \n",
              "3  The so-called \"CIA torture report\" has heighte...  \n",
              "4  President Donald Trump, a former beauty pagean...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-06bacc73-6285-41a2-84bf-8c7c4a30a3e8\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>example_id</th>\n",
              "      <th>label</th>\n",
              "      <th>url</th>\n",
              "      <th>annotations</th>\n",
              "      <th>claim</th>\n",
              "      <th>person</th>\n",
              "      <th>venue</th>\n",
              "      <th>justification</th>\n",
              "      <th>full_article</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>8057719209342304749</td>\n",
              "      <td>false</td>\n",
              "      <td>https://www.politifact.com/factchecks/2020/apr...</td>\n",
              "      <td>[{'questions': ['Is voting fraud widespread in...</td>\n",
              "      <td>With voting by mail, “you get thousands and th...</td>\n",
              "      <td>Donald Trump</td>\n",
              "      <td>stated on April 7, 2020 in a press briefing:</td>\n",
              "      <td>Trump said that with voting by mail, \"you get ...</td>\n",
              "      <td>The daily White House briefings about coronavi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-3333998957238197422</td>\n",
              "      <td>barely-true</td>\n",
              "      <td>https://www.politifact.com/factchecks/2019/mar...</td>\n",
              "      <td>[{'questions': ['Was the federal aid given by ...</td>\n",
              "      <td>\"I’ve already traveled to Washington, D.C., an...</td>\n",
              "      <td>Ron DeSantis</td>\n",
              "      <td>stated on March 5, 2019 in his State of the St...</td>\n",
              "      <td>DeSantis said, \"I’ve already traveled to Washi...</td>\n",
              "      <td>Editor’s note, March 10 12:55 p.m.: Two days a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-5816336384767541299</td>\n",
              "      <td>barely-true</td>\n",
              "      <td>https://www.politifact.com/factchecks/2016/nov...</td>\n",
              "      <td>[{'questions': ['Is this ban directly linked t...</td>\n",
              "      <td>Says that when San Francisco banned plastic gr...</td>\n",
              "      <td>James Quintero</td>\n",
              "      <td>stated on October 10, 2016 in a panel discussi...</td>\n",
              "      <td>Quintero said that when San Francisco banned p...</td>\n",
              "      <td>Reused grocery bags made Californians sick, a ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>7968458905312541095</td>\n",
              "      <td>true</td>\n",
              "      <td>https://www.politifact.com/factchecks/2014/dec...</td>\n",
              "      <td>[{'questions': ['Is it true that  The United S...</td>\n",
              "      <td>The United States \"decided waterboarding was t...</td>\n",
              "      <td>Sheldon Whitehouse</td>\n",
              "      <td>stated on December 14, 2014 in a TV interview:</td>\n",
              "      <td>Sheldon Whitehouse said the United States \"dec...</td>\n",
              "      <td>The so-called \"CIA torture report\" has heighte...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-2095875040468818200</td>\n",
              "      <td>false</td>\n",
              "      <td>https://www.politifact.com/factchecks/2020/sep...</td>\n",
              "      <td>[{'questions': ['Has Trump been accused of wal...</td>\n",
              "      <td>Quotes Donald Trump as saying, “I’ll tell you ...</td>\n",
              "      <td>Viral image</td>\n",
              "      <td>stated on September 21, 2020 in a post on Face...</td>\n",
              "      <td>A Facebook post quotes President Trump as sayi...</td>\n",
              "      <td>President Donald Trump, a former beauty pagean...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-06bacc73-6285-41a2-84bf-8c7c4a30a3e8')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-06bacc73-6285-41a2-84bf-8c7c4a30a3e8 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-06bacc73-6285-41a2-84bf-8c7c4a30a3e8');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exploration Code, to be removed"
      ],
      "metadata": {
        "id": "wI1mKAbO2XfM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tmp = []\n",
        "# l1 = df.iloc[0]['annotations'][0]['questions']\n",
        "# l2 = df.iloc[0]['annotations'][1]['questions']\n",
        "# tmp.extend(l1)\n",
        "# tmp.extend(l2)\n",
        "\n",
        "# \"\".join(tmp)\n",
        "# tmp"
      ],
      "metadata": {
        "id": "u1oJtWUVxuR4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Select only the claim, justification and questions from the df\n",
        "dfFiltered = df.loc[ : ,['claim', 'justification']]\n",
        "dfFiltered['questions'] = np.nan\n",
        "\n",
        "#Combine all subquestions for each claim into a list\n",
        "for i in range(len(df)):\n",
        "  annotationList = df.loc[i, 'annotations']\n",
        "  questions = []\n",
        "  \n",
        "  for j in range(len(annotationList)):\n",
        "    questionSet = annotationList[j]['questions']\n",
        "    questions.extend(questionSet)\n",
        "  \n",
        "  dfFiltered['questions'][i] = questions\n",
        "\n",
        "dfFiltered.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        },
        "id": "Ke7yuDuePxe6",
        "outputId": "b902a3d8-0093-4243-b734-c3befd559c00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py:1732: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  self._setitem_single_block(indexer, value, name)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               claim  \\\n",
              "0  With voting by mail, “you get thousands and th...   \n",
              "1  \"I’ve already traveled to Washington, D.C., an...   \n",
              "2  Says that when San Francisco banned plastic gr...   \n",
              "3  The United States \"decided waterboarding was t...   \n",
              "4  Quotes Donald Trump as saying, “I’ll tell you ...   \n",
              "\n",
              "                                       justification  \\\n",
              "0  Trump said that with voting by mail, \"you get ...   \n",
              "1  DeSantis said, \"I’ve already traveled to Washi...   \n",
              "2  Quintero said that when San Francisco banned p...   \n",
              "3  Sheldon Whitehouse said the United States \"dec...   \n",
              "4  A Facebook post quotes President Trump as sayi...   \n",
              "\n",
              "                                           questions  \n",
              "0  [Is voting fraud widespread in the US?, Is the...  \n",
              "1  [Was the federal aid given by Trump for hurric...  \n",
              "2  [Is this ban directly linked to outbreaks of f...  \n",
              "3  [Is it true that  The United States \" decided ...  \n",
              "4  [Has Trump been accused of walking into a dres...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-18dff2f6-6707-4789-857a-4d18acd1e6e1\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>claim</th>\n",
              "      <th>justification</th>\n",
              "      <th>questions</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>With voting by mail, “you get thousands and th...</td>\n",
              "      <td>Trump said that with voting by mail, \"you get ...</td>\n",
              "      <td>[Is voting fraud widespread in the US?, Is the...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>\"I’ve already traveled to Washington, D.C., an...</td>\n",
              "      <td>DeSantis said, \"I’ve already traveled to Washi...</td>\n",
              "      <td>[Was the federal aid given by Trump for hurric...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Says that when San Francisco banned plastic gr...</td>\n",
              "      <td>Quintero said that when San Francisco banned p...</td>\n",
              "      <td>[Is this ban directly linked to outbreaks of f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>The United States \"decided waterboarding was t...</td>\n",
              "      <td>Sheldon Whitehouse said the United States \"dec...</td>\n",
              "      <td>[Is it true that  The United States \" decided ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Quotes Donald Trump as saying, “I’ll tell you ...</td>\n",
              "      <td>A Facebook post quotes President Trump as sayi...</td>\n",
              "      <td>[Has Trump been accused of walking into a dres...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-18dff2f6-6707-4789-857a-4d18acd1e6e1')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-18dff2f6-6707-4789-857a-4d18acd1e6e1 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-18dff2f6-6707-4789-857a-4d18acd1e6e1');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Further clean data by removing breakline tokens"
      ],
      "metadata": {
        "id": "g9r-3qAQpFHU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean data by removing breaklines \\n\n",
        "def removeBreaklines(df):\n",
        "  for j in range(len(df)):\n",
        "    qsnSeries = df[j]\n",
        "    for i in range(len(qsnSeries)):\n",
        "      qsnSeries[i] = qsnSeries[i].replace('\\n','')\n",
        "\n",
        "print(\"Before breakline removal:\\n\")\n",
        "print(dfFiltered.loc[100, \"questions\"])\n",
        "\n",
        "tmp = dfFiltered.loc[ : , 'questions']\n",
        "removeBreaklines(tmp)\n",
        "\n",
        "print(\"\\nAfter breakline removal:\\n\")\n",
        "dfFiltered.loc[100, \"questions\"]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E0yPIhlZo2-x",
        "outputId": "3978807f-c109-4454-d3a7-4947d8148b38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before breakline removal:\n",
            "\n",
            "['Did Hilary Clinton ever say that Rubio scares her?\\n', 'Have any democrats ever said that they were concerned about Rubio?', 'Has the person that this claim originates from always supported Hillary Clinton as president?', \"Did Hilary Clinton say there's only one candidate who scares her--Marco Rubio?\", 'Are Democrats concerned about Rubio?']\n",
            "\n",
            "After breakline removal:\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Did Hilary Clinton ever say that Rubio scares her?',\n",
              " 'Have any democrats ever said that they were concerned about Rubio?',\n",
              " 'Has the person that this claim originates from always supported Hillary Clinton as president?',\n",
              " \"Did Hilary Clinton say there's only one candidate who scares her--Marco Rubio?\",\n",
              " 'Are Democrats concerned about Rubio?']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Exporting Dataframe"
      ],
      "metadata": {
        "id": "_fxXYf_hCgi6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fileName = \"filtered_data/dev.csv\"\n",
        "dfFiltered.to_csv(fileName, index=False, encoding = 'utf-8-sig', header=True, )"
      ],
      "metadata": {
        "id": "ZgsKJc6cyOcQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Abstracting Dataframe Creation\n",
        "\n",
        "The above cells are abstracted into the filterData method. \n",
        "\n",
        "The following cell represents a condensed version suitable for abstraction into a standalone python file for dataframe creation"
      ],
      "metadata": {
        "id": "FNU3_gVPCmYD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "from pandas import json_normalize\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Clean data by removing breaklines \\n\n",
        "def removeBreaklines(df):\n",
        "  for j in range(len(df)):\n",
        "    qsnSeries = df[j]\n",
        "    for i in range(len(qsnSeries)):\n",
        "      qsnSeries[i] = qsnSeries[i].replace('\\n','')\n",
        "\n",
        "# Selects and returns clean data\n",
        "def filterData(sourcePath, destPath):\n",
        "  \n",
        "  #Load jsonl file\n",
        "  with open(sourcePath, 'r') as jsonl_file:\n",
        "      jsonl_list = list(jsonl_file)\n",
        "\n",
        "  #listOfRows is a list of dict, each dict containing data for a row in the df with keys = column name\n",
        "  listOfRows = []\n",
        "  for entry in jsonl_list:\n",
        "      loadedEntry = json.loads(entry)\n",
        "      listOfRows.append(loadedEntry)\n",
        "\n",
        "  #Convert list of dict into a df\n",
        "  df = pd.DataFrame(listOfRows)\n",
        "\n",
        "  #Select only the claim, justification and questions from the original df\n",
        "  dfFiltered = df.loc[ : ,['claim', 'justification']]\n",
        "  dfFiltered['questions'] = np.nan\n",
        "\n",
        "  #Combine all subquestions for each claim into a list\n",
        "  for i in tqdm(range(len(df))):\n",
        "    annotationList = df.loc[i, 'annotations']\n",
        "    questions = []\n",
        "    \n",
        "    for j in range(len(annotationList)):\n",
        "      questionSet = annotationList[j]['questions']\n",
        "      questions.extend(questionSet)\n",
        "    \n",
        "    dfFiltered['questions'][i] = questions\n",
        "\n",
        "  #Remove breakline tokens \n",
        "  tmp = dfFiltered.loc[ : , 'questions']\n",
        "  removeBreaklines(tmp)\n",
        "\n",
        "  #Export filtered df to csv\n",
        "  dfFiltered.to_csv(destPath, index=False, encoding = 'utf-8-sig', header=True, )\n",
        "\n",
        "  #Export filtered df to json\n",
        "  # dfFiltered.to_json(destPath, orient='table', index=False)\n",
        "\n",
        "#File Paths\n",
        "devSource = \"./reconstructed_data/dev.jsonl\"\n",
        "devDest = \"./filtered_data/dev.csv\"\n",
        "\n",
        "trainSource = \"./reconstructed_data/train.jsonl\"\n",
        "trainDest = \"./filtered_data/train.csv\"\n",
        "\n",
        "testSource = \"./reconstructed_data/test.jsonl\"\n",
        "testDest = \"./filtered_data/test.csv\"\n",
        "\n",
        "#Data Extraction\n",
        "print(\"Creating dev.csv\")\n",
        "filterData(devSource, devDest)\n",
        "\n",
        "print(\"\\nCreating train.csv\")\n",
        "filterData(trainSource, trainDest)\n",
        "\n",
        "print(\"\\nCreating test.csv\")\n",
        "filterData(testSource, testDest)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ilvccqjG_OGQ",
        "outputId": "e58cd76e-3d1d-4cbc-c8ba-08a8bf400b87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating dev.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 197/197 [00:00<00:00, 18918.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Creating train.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 793/793 [00:00<00:00, 36134.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Creating test.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 200/200 [00:00<00:00, 18152.45it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#File Paths\n",
        "devSource = \"./reconstructed_data/dev.jsonl\"\n",
        "devDest = \"./filtered_data/dev.json\"\n",
        "\n",
        "#Data Extraction\n",
        "print(\"Creating dev.json\")\n",
        "filterData(devSource, devDest)"
      ],
      "metadata": {
        "id": "F33MNoSvFYcy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45a68f08-f9d8-49fb-bb40-a28920a2604a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating dev.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 197/197 [00:00<00:00, 26484.96it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CUoo5GEnp32z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}